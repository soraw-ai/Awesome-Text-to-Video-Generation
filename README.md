# Awesome-Video-Generation [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
> Topics about: <br>
> `Text-to-Image`, `Text-to-Video`, `Image-to-Video`

## Table of Content
* [Text-to-Image](#text_to_image)
* [Text-to-Video](#text_to_video)
* [Image-to-Video](#image_to_video)
* [Video-to-Video](#video_to_video)

## <a name="text_to_image"></a> Text-to-Image
- **Scalable Diffusion Models with Transformers** `Sequential Images` <br>
  Team: UC Berkeley, NYU. <br>
  *William Peebles, Saining Xie* <br>
  **ICCV'23(Oral)**, arXiv, 2022.12 [[Paper](https://arxiv.org/abs/2212.09748)], [[PDF](https://arxiv.org/pdf/2212.09748.pdf)], [[Code](https://github.com/facebookresearch/DiT)], [[Pretrained Model](https://github.com/facebookresearch/DiT)], [[Home Page](https://www.wpeebles.com/DiT.html)] <br>

## <a name="text_to_video"></a> Text-to-Video
- **World Model on Million-Length Video And Language With RingAttention** `Long Video` <br>
  Team: UC Berkeley. <br>
  *Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel* <br>
  arXiv, 2024.02 [[Paper](https://arxiv.org/abs/2402.08268)], [[PDF](https://arxiv.org/pdf/2402.08268.pdf)], [[Code](https://github.com/LargeWorldModel/LWM)], [[Pretrained Model](https://huggingface.co/LargeWorldModel)], [[Home Page](https://largeworldmodel.github.io/)] <br>
- **VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models** <br>
  Team: Tencent AI Lab. <br>
  *Haoxin Chen, Yong Zhang, Xiaodong Cun, et al., Ying Shan* <br>
  arXiv, 2024.01 [[Paper](https://arxiv.org/abs/2401.09047)], [[PDF](https://arxiv.org/pdf/2401.09047.pdf)], [[Code](https://github.com/AILab-CVC/VideoCrafter?tab=readme-ov-file)], [[Pretrained Model](https://github.com/AILab-CVC/VideoCrafter?tab=readme-ov-file)], [[Home Page](https://ailab-cvc.github.io/videocrafter2/)] <br>
- **VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation** <br>
  Team: Peking University, Microsoft Research. <br>
  *Wenjing Wang, Huan Yang, Zixi Tuo, et al., Jiaying Liu* <br>
  arxiv, 2023.12[[Paper](https://arxiv.org/abs/2312.14125)], [[PDF](https://arxiv.org/pdf/2312.14125.pdf)] <br>
- **Photorealistic Video Generation with Diffusion Models** <br>
  Team: Stanford University Fei-Fei Li, Google. <br>
  *Agrim Gupta, Lijun Yu, Kihyuk Sohn, et al., José Lezama* <br>
  arXiv, 2023.12 [[Paper](https://arxiv.org/abs/2312.06662)], [[PDF](https://arxiv.org/pdf/2312.06662.pdf)], [[Home Page](https://walt-video-diffusion.github.io/)] <br>
- **Make Pixels Dance: High-Dynamic Video Generation** <br>
  Team: ByteDance. <br>
  *Yan Zeng, Guoqiang Wei, Jiani Zheng, et al., Hang Li* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.10982)], [[PDF](https://arxiv.org/pdf/2311.10982.pdf)], [[Home Page](https://makepixelsdance.github.io/)], [[Demo](https://www.youtube.com/watch?v=QERmPmCg9aQ)] <br>
- **Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning** <br>
  Team: Meta. <br>
  *Rohit Girdhar, Mannat Singh, Andrew Brown, et al., Ishan Misra* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.10709)], [[PDF](https://arxiv.org/pdf/2311.10709.pdf)], [[Home Page](https://emu-video.metademolab.com/)], [[Demo](https://emu-video.metademolab.com/#/demo)] <br>
- **LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models** <br>
  Team: Shanghai Artificial Intelligence Laboratory. <br>
  *Yaohui Wang, Xinyuan Chen, Xin Ma, et al., Ziwei Liu* <br>
  arXiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.15103)], [[PDF](https://arxiv.org/pdf/2309.15103.pdf)], [[Code](https://github.com/Vchitect/LaVie)], [[Home Page](https://vchitect.github.io/LaVie-project/)] <br>
- **Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation** <br>
  Team: Huawei. <br>
  *Jiaxi Gu, Shicong Wang, Haoyu Zhao, et al., Hang Xu* <br>
  arXiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.03549)], [[PDF](https://arxiv.org/pdf/2309.03549.pdf)], [[Code](https://github.com/anonymous0x233/ReuseAndDiffuse)], [[Home Page](https://anonymous0x233.github.io/ReuseAndDiffuse/)] <br>
- **Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator** <br>
  Team: School of Information Science and Technology, ShanghaiTech University. <br>
  *Hanzhuo Huang, Yufan Feng, Cheng Shi, et al., Sibei Yang* <br>
  **NeurIPS'24**, arxiv, 2023.9[[Paper](https://arxiv.org/abs/2309.14494)], [[PDF](https://arxiv.org/pdf/2309.14494.pdf)], [[Home Page](https://github.com/showlab/Tune-A-Video)] <br>
- **Show-1: Marrying pixel and latent diffusion models for text-to-video generation.** <br>
  Team: Show Lab, National University of Singapor <br>
  *David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, et al., Mike Zheng Shou* <br>
  arxiv, 2023.9 [[Paper](https://arxiv.org/abs/2309.15818)], [[PDF](https://arxiv.org/pdf/2309.15818.pdf)], [[Home Page](https://showlab.github.io/Show-1/)],[[Code](https://github.com/showlab/Show-1)], [[Pretrained Model](https://huggingface.co/spaces/showlab/Show-1)] <br>
- **GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER** <br>
  Team: Institute of Automation, Chinese Academy of Sciences (CASIA). <br>
  *Mingzhen Sun, Weining Wang, Zihan Qin, et al., Jing Liu* <br>
  NeurIPS'23, arXiv, 2023.09 [[Paper](https://arxiv.org/abs/2309.13274)], [[PDF](https://arxiv.org/pdf/2309.13274.pdf)], [[Code](https://github.com/iva-mzsun/GLOBER)], [[Home Page](https://iva-mzsun.github.io/GLOBER)], [[Demo](https://iva-mzsun.github.io/GLOBER)] <br>
- **Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models** <br>
  Team: National University of Singapore. <br>
  *Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Tat-Seng Chua* <br>
  arXiv, 2023.08 [[Paper](https://arxiv.org/abs/2308.13812)], [[PDF](https://arxiv.org/pdf/2308.13812.pdf)], [[Code](https://github.com/scofield7419/Dysen-VDM)] <br>
- **ModelScope Text-to-Video Technical Report** <br>
  Team: Alibaba Group. <br>
  *Jiuniu Wang, Hangjie Yuan, Dayou Chen, et al., Shiwei Zhang* <br>
  arXiv, 2023.08 [[Paper](https://arxiv.org/abs/2308.06571)], [[PDF](https://arxiv.org/pdf/2308.06571.pdf)], [[Home Page](https://modelscope.cn/models/iic/text-to-video-synthesis/summary)], [[Demo](https://huggingface.co/spaces/ali-vilab/modelscope-text-to-video-synthesis)] <br>
- **InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation** <br>
  Team: Shanghai AI Laboratory. <br>
  *Yi Wang, Yinan He, Yizhuo Li, et al., Yu Qiao* <br>
  ICLR'24, arXiv, 2023.07 [[Paper](https://arxiv.org/abs/2307.06942)], [[PDF](https://arxiv.org/pdf/2307.06942.pdf)], [[Code](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid)], [[Pretrained Model](https://huggingface.co/OpenGVLab/ViCLIP)] <br>
- **Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation** <br>
  Team: HKUST. <br>
  *Yingqing He, Menghan Xia, Haoxin Chen, et al., Qifeng Chen* <br>
  arXiv, 2023.07 [[Paper](https://arxiv.org/abs/2307.06940)], [[PDF](https://arxiv.org/pdf/2307.06940.pdf)], [[Code](https://github.com/AILab-CVC/Animate-A-Story)], [[Home Page](https://ailab-cvc.github.io/Animate-A-Story/)], [[Demo](https://ailab-cvc.github.io/Animate-A-Story/)] <br>
- **Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance** <br>
  Team: CUHK. <br>
  *Jinbo Xing, Menghan Xia, Yuxin Liu, et al., Tien-Tsin Wong* <br>
  arXiv, 2023.06 [[Paper](https://arxiv.org/abs/2306.00943)], [[PDF](https://arxiv.org/pdf/2306.00943.pdf)], [[Code](https://github.com/AILab-CVC/Make-Your-Video)], [[Pretrained Model](https://huggingface.co/Doubiiu/Make-Your-Video/blob/main/model.ckpt)], [[Home Page](https://doubiiu.github.io/projects/Make-Your-Video/)] <br>
- **VideoComposer: Compositional Video Synthesis with Motion Controllability** <br>
  Team: Alibaba Group. <br>
  *Xiang Wang, Hangjie Yuan, Shiwei Zhang, et al., Jingren Zhou* <br>
  NeurIPS'23, arXiv, 2023.06 [[Paper](https://arxiv.org/abs/2306.02018)], [[PDF](https://arxiv.org/pdf/2306.02018.pdf)], [[Code](https://github.com/ali-vilab/videocomposer)], [[Pretrained Model](https://www.modelscope.cn/models/iic/VideoComposer/summary)], [[Home Page](https://videocomposer.github.io/)] <br>
- **VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation** <br>
  Team: University of Chinese Academy of Sciences (UCAS), Alibaba Group. <br>
  *Zhengxiong Luo, Dayou Chen, Yingya Zhang, et al., Tieniu Tan* <br>
  CVPR'23, arXiv, 2023.06 [[Paper](https://arxiv.org/abs/2303.08320)], [[PDF](https://arxiv.org/pdf/2303.08320.pdf)]<br>
- **Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models** <br>
  Team: University of Maryland. <br>
  *Songwei Ge, Seungjun Nah, Guilin Liu, et al., Yogesh Balaji* <br>
  ICCV'23, arXiv, 2023.05 [[Paper](https://arxiv.org/abs/2305.10474)], [[PDF](https://arxiv.org/pdf/2305.10474.pdf)], [[Home Page](https://research.nvidia.com/labs/dir/pyoco/)] <br>
- **VideoPoet: A Large Language Model for Zero-Shot Video Generation** <br>
  Team: Google Research <br>
  *Dan Kondratyuk, Lijun Yu, Xiuye Gu, et al., Lu Jiang* <br>
  arxiv, 2023.5[[Paper](https://arxiv.org/abs/2305.10874)], [[PDF](https://arxiv.org/pdf/2305.10874.pdf)], [[Home Page](https://sites.research.google/videopoet/)], [[Blog](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html)] <br>
- **VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning** <br>
  Team: Tsinghua University, Beijing Film Academy <br>
  *Hong Chen, Xin Wang, Guanning Zeng, et al., WenwuZhu* <br>
  arxiv, 2023.5[[Paper](https://arxiv.org/abs/2311.00990)], [[PDF](https://arxiv.org/pdf/2311.00990.pdf)], [[Home Page](https://videodreamer23.github.io/)],[[Code](https://github.com/videodreamer23/videodreamer23.github.io)] <br>
- **Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation** <br>
  Team: University of Rochester, Meta. <br>
  *Jie An, Songyang Zhang, Harry Yang, et al., Xi Yin* <br>
  arXiv, 2023.04 [[Paper](https://arxiv.org/abs/2304.08477)], [[PDF](https://arxiv.org/pdf/2304.08477.pdf)], [[Home Page](https://latent-shift.github.io/)] <br>
- **Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models** <br>
  Team: NVIDIA. <br>
  *Andreas Blattmann, Robin Rombach, Huan Ling, et al., Karsten Kreis* <br>
  CVPR'23, arXiv, 2023.04 [[Paper](https://arxiv.org/abs/2304.08818)], [[PDF](https://arxiv.org/pdf/2304.08818.pdf)], [[Home Page](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)] <br>
- **Structure and Content-Guided Video Synthesis with Diffusion Models** <br>
  Team: Runway <br>
  *Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis* <br>
  ICCV'23, arXiv, 2023.02 [[Paper](https://arxiv.org/abs/2302.03011)], [[PDF](https://arxiv.org/pdf/2302.03011.pdf)], [[Home Page](https://research.runwayml.com/gen1)] <br>
- **MagicVideo: Efficient Video Generation With Latent Diffusion Models** <br>
  Team: ByteDance Inc. <br>
  *Daquan Zhou, Weimin Wang, Hanshu Yan, et al., Jiashi Feng* <br>
  arXiv, 2022.11 [[Paper](https://arxiv.org/abs/2211.11018)], [[PDF](https://arxiv.org/pdf/2211.11018.pdf)], [[Home Page](https://magicvideo.github.io/)] <br>
- **Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation** <br>
  Team: Show Lab, National University of Singapore. <br>
  *Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Mike Zheng Shou et al* <br>
  **ICCV'23**, arxiv, 2022.12[[Paper](https://arxiv.org/abs/2212.11565)], [[PDF](https://arxiv.org/pdf/2212.11565.pdf)], [[Code](https://github.com/showlab/Tune-A-Video)], [[Pretrained Model](https://huggingface.co/Tune-A-Video-library)]  <br>

- **Latent Video Diffusion Models for High-Fidelity Long Video Generation** `Long Video` <br>
  Team: HKUST, Tencent AI Lab. <br>
  *Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Qifeng Chen* <br>
  arXiv, 2022.10 [[Paper](https://arxiv.org/abs/2211.13221)], [[PDF](https://arxiv.org/pdf/2211.13221.pdf)], [[Code](https://github.com/YingqingHe/LVDM)], [[Home Page](https://yingqinghe.github.io/LVDM/)] <br>
- **Phenaki: Variable Length Video Generation From Open Domain Textual Description** <br>
  Team: Google. <br>
  *Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, et al., Dumitru Erhan* <br>
  ICLR'23, arXiv, 2022.10 [[Paper](https://arxiv.org/abs/2210.02399)], [[PDF](https://arxiv.org/pdf/2210.02399.pdf)], [[Home Page](https://sites.research.google/phenaki/)] <br>
- **Imagen Video: High Definition Video Generation with Diffusion Models** <br>
  Team: Google. <br>
  *Jonathan Ho, William Chan, Chitwan Saharia, et al., Tim Salimans* <br>
  arXiv, 2022.10 [[Paper](https://arxiv.org/abs/2210.02303)], [[PDF](https://arxiv.org/pdf/2210.02303.pdf)], [[Home Page](https://imagen.research.google/video/)] <br>
- **Make-A-Video: Text-to-Video Generation without Text-Video Data** <br>
  Team: Meta AI. <br>
  *Uriel Singer, Adam Polyak, Thomas Hayes, et al., Yaniv Taigman* <br>
  arxiv, 2022.9 [[Paper](https://arxiv.org/abs/2209.14792)], [[PDF](https://arxiv.org/pdf/2209.14792.pdf)], [[Code](https://github.com/SooLab/Free-Bloom)]<br>
- **CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers** <br>
  Team: Tsinghua University. <br>
  *Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang* <br>
  ICLR'23, arXiv, 2022.05 [[Paper](https://arxiv.org/abs/2205.15868)], [[PDF](https://arxiv.org/pdf/2205.15868.pdf)], [[Code](https://github.com/THUDM/CogVideo)], [[Home Page](https://models.aminer.cn/cogvideo/)], [[Demo](https://huggingface.co/spaces/THUDM/CogVideo)] <br>
- **NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis** `Long Video` <br>
  Team: Microsoft. <br>
  *Chenfei Wu, Jian Liang, Xiaowei Hu, et al., Nan Duan* <br>
  NeurIPS'22, arXiv, 2022.02 [[Paper](https://arxiv.org/abs/2207.09814)], [[PDF](https://arxiv.org/pdf/2207.09814.pdf)], [[Code](https://github.com/microsoft/NUWA)], [[Home Page](https://nuwa-infinity.microsoft.com/#/)] <br>
- **NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion** <br>
  Team: Microsoft. <br>
  *Chenfei Wu, Jian Liang, Lei Ji, et al., Nan Duan* <br>
  ECCV'22, arXiv, 2021.11 [[Paper](https://arxiv.org/abs/2111.12417)], [[PDF](https://arxiv.org/pdf/2111.12417.pdf)], [[Code](https://github.com/microsoft/NUWA)]<br>
- **GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions** <br>
  Team: Microsoft, Duke University. <br>
  *Chenfei Wu, Lun Huang, Qianxi Zhang, et al., Nan Duan* <br>
  arXiv, 2021.04 [[Paper](https://arxiv.org/abs/2104.14806)], [[PDF](https://arxiv.org/pdf/2104.14806.pdf)] <br>

## <a name="image_to_video"></a> Image-to-Video
- **Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets** <br>
  Team: Stability AI. <br>
  *Andreas Blattmann, Tim Dockhorn, Sumith Kulal, et al., Robin Rombach* <br>
  arXiv, 2023.11 [[Paper](https://arxiv.org/abs/2311.15127)], [[PDF](https://arxiv.org/pdf/2311.15127.pdf)], [[Code](https://github.com/Stability-AI/generative-models)] <br>

## <a name="video_to_video"></a> Video-to-Video
- **Long video generation with time-agnostic vqgan and time-sensitive transformer** <br>
  Team: Meta AI. <br>
  *Songwei Ge, Thomas Hayes, Harry Yang, et al., Devi Parikh* <br>
  **ECCV'22** arXiv, 2022.4 [[Paper](https://arxiv.org/abs/2204.03638)], [[PDF](https://arxiv.org/pdf/2204.03638.pdf)], [[Home Page](https://songweige.github.io/projects/tats/)], [[Code](https://github.com/SongweiGe/TATS)] <br>



----
## Acknowledgement
- [Awesome Text-to-Video Generation](https://github.com/feifeiobama/Awesome-Text-to-Video-Generation)



## Citation
If you find this repository useful, please consider citing this list:
```
@misc{rui2023videogenerationlist,
    title = {Awesome-Video-Generation},
    author = {Rui Sun},
    journal = {GitHub repository},
    url = {https://github.com/soraw-ai/Awesome-Video-Generation},
    year = {2024},
}
```

## References

